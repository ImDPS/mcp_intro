{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from pydantic_ai import Agent\n",
    "# from pydantic_ai.models.vertexai import VertexAIModel\n",
    "# from utils.logger import logger  # Ensure this is correctly set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an agent with the model\n",
    "agent = Agent(\n",
    "    'gemini-1.5-flash'\n",
    ")\n",
    "\n",
    "# Define a user query\n",
    "user_query = \"Tell me a joke.\"\n",
    "\n",
    "messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StreamedRunResult(_all_messages=[ModelRequest(parts=[UserPromptPart(content='Tell me a joke.', timestamp=datetime.datetime(2025, 3, 27, 11, 38, 30, 935019, tzinfo=datetime.timezone.utc), part_kind='user-prompt')], kind='request')], _new_message_index=0, _usage_limits=UsageLimits(request_limit=50, request_tokens_limit=None, response_tokens_limit=None, total_tokens_limit=None), _stream_response=GeminiStreamedResponse(_parts_manager=ModelResponsePartsManager(_parts=[TextPart(content='Why', part_kind='text')], _vendor_id_to_part_index={}), _event_iterator=<async_generator object GeminiStreamedResponse._get_event_iterator at 0x107743ab0>, _usage=Usage(requests=0, request_tokens=6, response_tokens=0, total_tokens=6, details=None), _model_name='gemini-1.5-flash', _content=bytearray(b'[{\\n  \"candidates\": [\\n    {\\n      \"content\": {\\n        \"parts\": [\\n          {\\n            \"text\": \"Why\"\\n          }\\n        ],\\n        \"role\": \"model\"\\n      }\\n    }\\n  ],\\n  \"usageMetadata\": {\\n    \"promptTokenCount\": 6,\\n    \"totalTokenCount\": 6,\\n    \"promptTokensDetails\": [\\n      {\\n        \"modality\": \"TEXT\",\\n        \"tokenCount\": 6\\n      }\\n    ]\\n  },\\n  \"modelVersion\": \"gemini-1.5-flash\"\\n}\\n,\\r\\n{\\n  \"candidates\": [\\n    {\\n      \"content\": {\\n        \"parts\": [\\n          {\\n            \"text\": \"'), _stream=<async_generator object Response.aiter_bytes at 0x10765f480>, _timestamp=datetime.datetime(2025, 3, 27, 11, 38, 31, 554820, tzinfo=datetime.timezone.utc)), _result_schema=None, _run_ctx=RunContext(deps=None, model=GeminiModel(), usage=Usage(requests=1, request_tokens=None, response_tokens=None, total_tokens=None, details=None), prompt='Tell me a joke.', messages=[ModelRequest(parts=[UserPromptPart(content='Tell me a joke.', timestamp=datetime.datetime(2025, 3, 27, 11, 38, 30, 935019, tzinfo=datetime.timezone.utc), part_kind='user-prompt')], kind='request')], tool_call_id=None, tool_name=None, retry=0, run_step=1), _result_validators=[], _result_tool_name=None, _on_complete=<function Agent.run_stream.<locals>.on_complete at 0x10761cea0>, _initial_run_ctx_usage=Usage(requests=1, request_tokens=None, response_tokens=None, total_tokens=None, details=None), is_complete=False)\n"
     ]
    }
   ],
   "source": [
    "async with agent.run_stream(\n",
    "\n",
    "    user_query, message_history=messages\n",
    ") as result:\n",
    "    print(result)\n",
    "    curr_message = \"\"\n",
    "    async for message in result.stream_text(delta=True):\n",
    "        curr_message += message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Why don't scientists trust atoms? \\n\\nBecause they make up everything!\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MCP Virtual Environment",
   "language": "python",
   "name": "mcpvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
